{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dacccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from arabert.preprocess import ArabertPreprocessor\n",
    "from farasa.segmenter import FarasaSegmenter\n",
    "import torch\n",
    "import arabicstopwords.arabicstopwords as ast\n",
    "import string\n",
    "from html.parser import HTMLParser\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a35e11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name= 'aubmindlab/bert-base-arabertv02'  \n",
    "tokenizer =AutoTokenizer.from_pretrained(model_name) \n",
    "model=AutoModel.from_pretrained(model_name,output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c70a21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    stop_words = ast.stopwords_list()\n",
    "    \n",
    "    return ' '.join(word for word in text.split() if word not in stop_words)\n",
    "def strip_tags(html):\n",
    "    html=str(html)\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()\n",
    "\n",
    "def text_to_tokens(text):\n",
    "    return tokenizer.tokenize(text)\n",
    "\n",
    "def embedding(text):\n",
    "    text = remove_stop_words(remove_punctuation(text))\n",
    "    inputs = tokenizer.encode_plus(text,return_tensors='pt')\n",
    "    outputs = model(inputs['input_ids'],inputs['attention_mask'],inputs['token_type_ids'])\n",
    "    return string,text,outputs['last_hidden_state'].detach().numpy()[0][0].tolist()\n",
    "    \n",
    "# tokens =text_to_tokens(remove_stop_words(text)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2279dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading json products\n",
    "dir_path=r'/var/www/html/Salla/New/'\n",
    "products_data=[]\n",
    "for path in os.listdir(dir_path):\n",
    "    if os.path.isfile(os.path.join(dir_path, path)):\n",
    "        try:\n",
    "            file = open(os.path.join(dir_path, path),'r')\n",
    "            content = json.load(file)\n",
    "            key_string=list(content.keys())[0]\n",
    "            for product in content['SELECT smsp.id as product_id, smsp.name as product_name,smsc.id as category_id ,\\r\\nsmsc.name as category_name,smsp.description as product_description, smsb.name as brand_name\\r\\nFROM salla_reports.stg_mysql_salla__products smsp \\r\\nJOIN stg_mysql_salla__product_categories smspc  on stg_mysql_salla__product_categories.product_id = smsp.id \\r\\nJOIN stg_mysql_salla__categories smsc on stg_mysql_salla__product_categories.category_id = stg_mysql_salla__categories.id \\r\\nJOIN stg_mysql_salla__brands smsb on smsb.id = smsp.brand_id ']:\n",
    "                data=product\n",
    "                data['product_description']=strip_tags(data['product_description'])\n",
    "                data['product_description_vector'] = embedding(data['product_description'])\n",
    "                data['product_name_vector'] = embedding(data['name'])\n",
    "                product_id=product['product_id']\n",
    "                products_data.append({\"index\": {\"_index\": \"products_semantic\", \"_id\": product_id}})\n",
    "                products_data.append(data)\n",
    "                if len(products_data)>=1000:\n",
    "                    resp = client.bulk(body=products_data)\n",
    "                    products_data=[]\n",
    "                    print(\"1000 Indexed\")\n",
    "                \n",
    "        except:\n",
    "            print(\"Error Parsing\")\n",
    "\n",
    "resp = client.bulk(body=products_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "63ba4abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text1 = \"ولن نبالغ إذا قلنا: إن 'هاتف' أو 'كمبيوتر المكتب' في زمننا هذا ضروري\"\n",
    "# text2 = \"إن 'هاتف' أو 'كمبيوتر المكتب' في زمننا هذا ضروري\"\n",
    "\n",
    "text1 = \"ايفون 17\"\n",
    "text2 = \"كفر جوالات\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "11a8eff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "ae7ca949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.82694749424007\n"
     ]
    }
   ],
   "source": [
    "# import required libraries\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    " \n",
    "# define two lists or array\n",
    "nan, nan, A = embedding(text1)\n",
    "nan, nan, B  = embedding(text2)\n",
    "\n",
    "\n",
    "# compute cosine similarity\n",
    "cosine = np.dot(A,B)/(norm(A)*norm(B))\n",
    "print(\"Cosine Similarity:\", cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb35f51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
